{"cells":[{"cell_type":"markdown","metadata":{"id":"hLcWRu-jp7gM"},"source":["# Flower Classfication with CNN"]},{"cell_type":"markdown","metadata":{"id":"B_MoQiztpxcK"},"source":["## Overview\n","\n","We design and train a deep convolutional network from scratch to predict the class label of a flower image."]},{"cell_type":"markdown","metadata":{"id":"giUId1Naqacs"},"source":["##  Versions of used packages\n","`python 3.8.11`, `torch==1.8.2` and `torchvision==0.9.2`"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27565,"status":"ok","timestamp":1636891526268,"user":{"displayName":"Da Bundun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08813202316216447423"},"user_tz":-480},"id":"Vuw-gNvjqcYe","outputId":"69932ade-0cc6-4518-810d-ef1d346b89b9"},"outputs":[{"name":"stdout","output_type":"stream","text":["python 3.7.12 (default, Sep 10 2021, 00:21:48) \n","torch 1.10.0+cu111\n","torchvision 0.11.1+cu111\n"]}],"source":["import sys\n","import torch\n","import torchvision\n","print('python', sys.version.split('\\n')[0])\n","print('torch', torch.__version__)\n","print('torchvision', torchvision.__version__)"]},{"cell_type":"markdown","metadata":{"id":"OhdbdJOsrbxL"},"source":["# Dataset"]},{"cell_type":"markdown","metadata":{"id":"nPoSgD83teTQ"},"source":["We use [Flowers Recognition](https://www.kaggle.com/alxmamaev/flowers-recognition) dataset, which collected by Alexander Mamaev.\n","\n","Flowers Recognition dataset contains 4317 flower images.  \n","\n","The pictures are divided into five classes: \n","+ daisy\n","+ tulip\n","+ rose\n","+ sunflower\n","+ dandelion\n","\n","For each class there are about 800 photos. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21515,"status":"ok","timestamp":1636891554011,"user":{"displayName":"Da Bundun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08813202316216447423"},"user_tz":-480},"id":"cCXepUIVe5iJ","outputId":"14be61fe-2790-48bc-9e09-a43554c50747"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aGCZQxZSfONu"},"outputs":[],"source":["!unzip -qq ./drive/MyDrive/Flowers Recognition.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lYccv1qYzYUz"},"outputs":[],"source":["data_folder = 'Flowers Recognition'"]},{"cell_type":"markdown","metadata":{"id":"SYueKGdIHpho"},"source":["## Loading the dataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dwUE1E83_Iw8"},"outputs":[],"source":["import csv\n","import os\n","import numpy as np\n","from PIL import Image\n","import torch\n","\n","class FlowerData(torch.utils.data.Dataset):\n","    def __init__(self, csv_file, mode='train', transform=None):\n","        self.mode = mode # 'train', 'val' or 'test'\n","        self.data_list = []\n","        self.labels = []\n","        self.transform = transform\n","        \n","        with open(f'{data_folder}/{csv_file}', newline='') as csvfile:\n","            reader = csv.DictReader(csvfile)\n","            for row in reader:\n","                self.data_list.append(f\"{data_folder}/{row['file_path']}\")\n","                if mode != 'test':\n","                    self.labels.append(row['label'])\n","\n","    def __getitem__(self, index):\n","        data = Image.open(self.data_list[index])\n","        if self.transform is not None:\n","            data = self.transform(data)\n","        if self.mode == 'test':\n","            return data\n","        label = torch.tensor(int(self.labels[index]))\n","\n","        return data, label\n","\n","    def __len__(self):\n","        return len(self.data_list)"]},{"cell_type":"markdown","metadata":{"id":"5tQGLuVWnA-b"},"source":["### Data augmentation "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UIv1VyHXVNTo"},"outputs":[],"source":["from torchvision import transforms\n","\n","# For train\n","transforms_train = transforms.Compose(\n","    [transforms.Resize((285, 285)),\n","     transforms.RandomHorizontalFlip(p=0.5),\n","     transforms.RandomVerticalFlip(p=0.5),\n","     transforms.RandomRotation((-180, 180)),\n","     transforms.RandomCrop(256, padding=2),\n","     transforms.ToTensor(),\n","     transforms.Normalize((0.48, 0.45, 0.46),(0.25, 0.25, 0.25))]\n",")\n","\n","\n","# For val, test\n","transforms_test = transforms.Compose(\n","    [transforms.Resize((285, 285)),\n","    transforms.CenterCrop(256),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.48, 0.45, 0.46),(0.25, 0.25, 0.25))]\n",")"]},{"cell_type":"markdown","metadata":{"id":"7rYptn_YJlFX"},"source":["### Instantiate dataset\n","\n","Let's instantiate three `FlowerData` class.\n","+ dataset_train: for training.\n","+ dataset_val: for validation.\n","+ dataset_test: for tesing."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LKjsKyHhZZc6"},"outputs":[],"source":["dataset_train = FlowerData('train.csv', mode='train', transform=transforms_train)\n","dataset_val = FlowerData('val.csv', mode='val', transform=transforms_test)\n","dataset_test = FlowerData('test.csv', mode='test', transform=transforms_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1636901363474,"user":{"displayName":"Da Bundun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08813202316216447423"},"user_tz":-480},"id":"ZAeQQEEISCJJ","outputId":"6b7fdf10-1b63-49be-951e-2ee6f6a4ffbb"},"outputs":[{"name":"stdout","output_type":"stream","text":["The first image's shape in dataset_train : torch.Size([3, 256, 256])\n","There are 1295 images in dataset_train.\n"]}],"source":["print(\"The first image's shape in dataset_train :\", dataset_train.__getitem__(0)[0].size())\n","print(\"There are\", dataset_train.__len__(), \"images in dataset_train.\")"]},{"cell_type":"markdown","metadata":{"id":"vORA6qkfIj1U"},"source":["### DataLoader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RmgA5nYT3XQZ"},"outputs":[],"source":["from torch.utils.data import DataLoader\n","\n","batch_size = 32\n","num_workers = 2\n","train_loader = DataLoader(dataset_train, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n","val_loader = DataLoader(dataset_val, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n","test_loader = DataLoader(dataset_test, batch_size=batch_size, num_workers=num_workers, shuffle=False)"]},{"cell_type":"markdown","metadata":{"id":"87KYcWknS95z"},"source":["# Build Model"]},{"cell_type":"markdown","metadata":{"id":"OH_4NKB9dsZ3"},"source":["### Convolutional Neural Network"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S_HxxEfdO2Ss"},"outputs":[],"source":["import torch.nn as nn \n","import torch.nn.functional as F\n","\n","class Your_CNN_Model(nn.Module): \n","    def __init__(self): \n","        super().__init__()\n","\n","\n","        # CNN model\n","        self.conv1 = nn.Conv2d(3, 32, 3, padding=2)\n","        self.conv2 = nn.Conv2d(32, 32, 3, padding=1)\n","\n","        self.conv3 = nn.Conv2d(32, 32, 3, padding=1)\n","        self.conv4 = nn.Conv2d(32, 64, 3, padding=1)  \n","\n","\n","        self.conv5 = nn.Conv2d(64, 64, 3, padding=1)\n","        self.conv6 = nn.Conv2d(64, 128, 3, padding=1)\n","        self.conv7 = nn.Conv2d(128, 128, 3, padding=1)\n","\n","\n","        self.conv8 = nn.Conv2d(128, 256, 3, padding=1)\n","        self.conv9 = nn.Conv2d(256, 256, 3, padding=1)\n","        self.conv10 = nn.Conv2d(256, 512, 3, padding=1)\n","\n","        self.fc1 = nn.Linear(512 * 16 * 16, 600)\n","        self.fc2 = nn.Linear(600, 200)\n","        self.fc3 = nn.Linear(200, 5)\n","\n","        self.dropout1 = nn.Dropout(0.4)\n","\n","\n","    def forward(self, x): \n","        if not isinstance(x, torch.Tensor):\n","            x = torch.Tensor(x)\n"," \n","        out = F.relu(self.conv1(x))\n","        out = F.max_pool2d(F.relu(self.conv2(out)), 2)\n","\n","        out = F.relu(self.conv3(out))\n","        out = F.max_pool2d(F.relu(self.conv4(out)), 2)\n","\n","        out = F.relu(self.conv5(out))\n","        out = F.relu(self.conv6(out))\n","        out = F.max_pool2d(F.relu(self.conv7(out)), 2)\n","\n","        out = F.relu(self.conv8(out))\n","        out = F.relu(self.conv9(out))\n","        out = F.max_pool2d(F.relu(self.conv10(out)), 2)\n","\n","        out = out.view(out.size(0), -1)\n","        out = torch.flatten(out, 1)\n","        out = F.relu(self.fc1(out))\n","        out = self.dropout1(out)\n","        out = F.relu(self.fc2(out))\n","        out = self.dropout1(out)\n","        out = self.fc3(out)\n","\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":928,"status":"ok","timestamp":1636901421708,"user":{"displayName":"Da Bundun","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08813202316216447423"},"user_tz":-480},"id":"lP55C9Xjydws","outputId":"cd86ef43-fa0d-4d32-e447-cf15fd28f109"},"outputs":[{"name":"stdout","output_type":"stream","text":["Your_CNN_Model(\n","  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n","  (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (conv3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (conv4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (conv5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (conv6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (conv7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (conv8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (conv9): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (conv10): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (fc1): Linear(in_features=131072, out_features=600, bias=True)\n","  (fc2): Linear(in_features=600, out_features=200, bias=True)\n","  (fc3): Linear(in_features=200, out_features=5, bias=True)\n","  (dropout1): Dropout(p=0.4, inplace=False)\n",")\n"]}],"source":["model = Your_CNN_Model()\n","model = model.cuda()\n","print(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wOBCTZmbzYU4"},"outputs":[],"source":["device = torch.device('cuda')\n","# device = torch.device('cpu')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TeWtAa1dzYU4"},"outputs":[],"source":["model = Your_CNN_Model()\n","model = model.to(device)\n","# print(model)"]},{"cell_type":"markdown","metadata":{"id":"FtOkPO6Ga0Fw"},"source":["### Define loss and optimizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IoePct00RIFY"},"outputs":[],"source":["import torch.nn as nn\n","import torch.optim as optim\n","\n","# Define loss and optmizer \n","criterion = nn.CrossEntropyLoss() \n","optimizer = optim.Adam(model.parameters(), lr = 0.0005)\n","\n","criterion = criterion.to(device)"]},{"cell_type":"markdown","metadata":{"id":"zle9KuFcbwMP"},"source":["### Train Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VM93brDshO6E"},"outputs":[],"source":["def train(input_data, model, criterion, optimizer):\n","    '''\n","    Argement:\n","    input_data -- iterable data, typr torch.utils.data.Dataloader is prefer\n","    model -- nn.Module, model contain forward to predict output\n","    criterion -- loss function, used to evaluate goodness of model\n","    optimizer -- optmizer function, method for weight updating\n","    '''\n","    model.train()\n","    loss_list = []\n","    total_count = 0\n","    acc_count = 0\n","    for images, labels in input_data:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        \n","\n","        # Forward, backward and optimize                                 \n","        optimizer.zero_grad()\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","      \n","\n","        # Get the counts of correctly classified images\n","        _, predicted = torch.max(outputs.data, 1)\n","        total_count += labels.size(0)\n","        acc_count += (predicted == labels).sum().item()\n","        loss_list.append(loss.item())\n","  \n","\n","    # Compute this epoch accuracy and loss\n","    acc = acc_count / total_count\n","    loss = sum(loss_list) / len(loss_list)\n","    return acc, loss"]},{"cell_type":"markdown","metadata":{"id":"KmDy1GTq_H2a"},"source":["#### Validate function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"USzbBgGEoTRu"},"outputs":[],"source":["def val(input_data, model, criterion):\n","    model.eval()\n","    \n","    loss_list = []\n","    total_count = 0\n","    acc_count = 0\n","    with torch.no_grad():\n","        for images, labels in input_data:\n","            images = images.to(device)\n","            labels = labels.to(device)\n","\n","            # Get the predicted result and loss\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total_count += labels.size(0)\n","            acc_count += (predicted == labels).sum().item()\n","            loss_list.append(loss.item())\n","\n","    acc = acc_count / total_count\n","    loss = sum(loss_list) / len(loss_list)\n","    return acc, loss"]},{"cell_type":"markdown","metadata":{"id":"knXu74jCiuxP"},"source":["## Training"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rcVulKkFJRtI","outputId":"c86f5832-a3a8-4ab9-de0d-bd1e70bdd6ab"},"outputs":[{"name":"stdout","output_type":"stream","text":["==================== Epoch 2 ====================\n","Train Acc: 0.230116 Train Loss: 1.603611\n","  Val Acc: 0.244186   Val Loss: 1.596244\n","==================== Epoch 4 ====================\n","Train Acc: 0.299614 Train Loss: 1.529226\n","  Val Acc: 0.318605   Val Loss: 1.496073\n","==================== Epoch 6 ====================\n","Train Acc: 0.322008 Train Loss: 1.465235\n","  Val Acc: 0.390698   Val Loss: 1.382733\n","==================== Epoch 8 ====================\n","Train Acc: 0.423166 Train Loss: 1.307843\n","  Val Acc: 0.474419   Val Loss: 1.258784\n","==================== Epoch 10 ====================\n","Train Acc: 0.481853 Train Loss: 1.159371\n","  Val Acc: 0.490698   Val Loss: 1.131325\n","==================== Epoch 12 ====================\n","Train Acc: 0.522780 Train Loss: 1.100544\n","  Val Acc: 0.506977   Val Loss: 1.123086\n","==================== Epoch 14 ====================\n","Train Acc: 0.547490 Train Loss: 1.070274\n","  Val Acc: 0.588372   Val Loss: 1.040220\n","==================== Epoch 16 ====================\n","Train Acc: 0.579151 Train Loss: 1.032661\n","  Val Acc: 0.600000   Val Loss: 1.046791\n","==================== Epoch 18 ====================\n","Train Acc: 0.579151 Train Loss: 1.022365\n","  Val Acc: 0.600000   Val Loss: 1.018018\n","==================== Epoch 20 ====================\n","Train Acc: 0.606178 Train Loss: 0.984274\n","  Val Acc: 0.627907   Val Loss: 0.978344\n","==================== Epoch 22 ====================\n","Train Acc: 0.614672 Train Loss: 0.953888\n","  Val Acc: 0.602326   Val Loss: 0.971960\n","==================== Epoch 24 ====================\n","Train Acc: 0.619305 Train Loss: 0.919924\n","  Val Acc: 0.637209   Val Loss: 0.927012\n","==================== Epoch 26 ====================\n","Train Acc: 0.623166 Train Loss: 0.901616\n","  Val Acc: 0.646512   Val Loss: 0.928468\n","==================== Epoch 28 ====================\n","Train Acc: 0.652510 Train Loss: 0.893372\n","  Val Acc: 0.625581   Val Loss: 0.955507\n","==================== Epoch 30 ====================\n","Train Acc: 0.646332 Train Loss: 0.869386\n","  Val Acc: 0.639535   Val Loss: 0.890399\n","==================== Epoch 32 ====================\n","Train Acc: 0.667954 Train Loss: 0.829602\n","  Val Acc: 0.637209   Val Loss: 0.975125\n","==================== Epoch 34 ====================\n","Train Acc: 0.665637 Train Loss: 0.848810\n","  Val Acc: 0.625581   Val Loss: 0.905151\n","==================== Epoch 36 ====================\n","Train Acc: 0.687259 Train Loss: 0.822237\n","  Val Acc: 0.646512   Val Loss: 0.915019\n","==================== Epoch 38 ====================\n","Train Acc: 0.687259 Train Loss: 0.767879\n","  Val Acc: 0.674419   Val Loss: 0.895913\n","==================== Epoch 40 ====================\n","Train Acc: 0.685714 Train Loss: 0.763338\n","  Val Acc: 0.662791   Val Loss: 0.883388\n","==================== Epoch 42 ====================\n","Train Acc: 0.681853 Train Loss: 0.764538\n","  Val Acc: 0.662791   Val Loss: 0.844135\n","==================== Epoch 44 ====================\n","Train Acc: 0.707336 Train Loss: 0.731168\n","  Val Acc: 0.655814   Val Loss: 0.887002\n","==================== Epoch 46 ====================\n","Train Acc: 0.677992 Train Loss: 0.774094\n","  Val Acc: 0.653488   Val Loss: 0.921814\n","==================== Epoch 48 ====================\n","Train Acc: 0.709653 Train Loss: 0.721676\n","  Val Acc: 0.655814   Val Loss: 0.883193\n","==================== Epoch 50 ====================\n","Train Acc: 0.714286 Train Loss: 0.698478\n","  Val Acc: 0.669767   Val Loss: 0.917209\n","==================== Epoch 52 ====================\n","Train Acc: 0.705019 Train Loss: 0.719238\n","  Val Acc: 0.690698   Val Loss: 0.832570\n","==================== Epoch 54 ====================\n","Train Acc: 0.715058 Train Loss: 0.729688\n","  Val Acc: 0.665116   Val Loss: 0.862480\n","==================== Epoch 56 ====================\n","Train Acc: 0.734363 Train Loss: 0.656466\n","  Val Acc: 0.660465   Val Loss: 0.850498\n","==================== Epoch 58 ====================\n","Train Acc: 0.713514 Train Loss: 0.680230\n","  Val Acc: 0.674419   Val Loss: 0.898813\n","==================== Epoch 60 ====================\n","Train Acc: 0.715830 Train Loss: 0.656354\n","  Val Acc: 0.679070   Val Loss: 0.871167\n","==================== Epoch 62 ====================\n","Train Acc: 0.718919 Train Loss: 0.674255\n","  Val Acc: 0.681395   Val Loss: 0.827041\n","==================== Epoch 64 ====================\n","Train Acc: 0.752124 Train Loss: 0.617689\n","  Val Acc: 0.706977   Val Loss: 0.861729\n","==================== Epoch 66 ====================\n","Train Acc: 0.744402 Train Loss: 0.637317\n","  Val Acc: 0.683721   Val Loss: 0.828370\n","==================== Epoch 68 ====================\n","Train Acc: 0.737452 Train Loss: 0.657155\n","  Val Acc: 0.665116   Val Loss: 0.896842\n","==================== Epoch 70 ====================\n","Train Acc: 0.726641 Train Loss: 0.631973\n","  Val Acc: 0.686047   Val Loss: 0.804471\n","==================== Epoch 72 ====================\n","Train Acc: 0.738224 Train Loss: 0.625063\n","  Val Acc: 0.674419   Val Loss: 0.842528\n","==================== Epoch 74 ====================\n","Train Acc: 0.742857 Train Loss: 0.604682\n","  Val Acc: 0.688372   Val Loss: 0.845533\n","==================== Epoch 76 ====================\n","Train Acc: 0.740541 Train Loss: 0.603113\n","  Val Acc: 0.688372   Val Loss: 0.832753\n","==================== Epoch 78 ====================\n","Train Acc: 0.739768 Train Loss: 0.598314\n","  Val Acc: 0.662791   Val Loss: 0.864437\n","==================== Epoch 80 ====================\n","Train Acc: 0.749035 Train Loss: 0.602287\n","  Val Acc: 0.697674   Val Loss: 0.892513\n","==================== Epoch 82 ====================\n","Train Acc: 0.745174 Train Loss: 0.636942\n","  Val Acc: 0.702326   Val Loss: 0.874108\n","==================== Epoch 84 ====================\n","Train Acc: 0.758301 Train Loss: 0.579286\n","  Val Acc: 0.660465   Val Loss: 0.882482\n","==================== Epoch 86 ====================\n","Train Acc: 0.759073 Train Loss: 0.588685\n","  Val Acc: 0.711628   Val Loss: 0.878228\n","==================== Epoch 88 ====================\n","Train Acc: 0.765251 Train Loss: 0.565598\n","  Val Acc: 0.704651   Val Loss: 0.835318\n","==================== Epoch 90 ====================\n","Train Acc: 0.779923 Train Loss: 0.555014\n","  Val Acc: 0.693023   Val Loss: 0.922462\n","==================== Epoch 92 ====================\n","Train Acc: 0.779151 Train Loss: 0.570081\n","  Val Acc: 0.688372   Val Loss: 0.971911\n","==================== Epoch 94 ====================\n","Train Acc: 0.770656 Train Loss: 0.542612\n","  Val Acc: 0.688372   Val Loss: 0.940554\n","==================== Epoch 96 ====================\n","Train Acc: 0.767568 Train Loss: 0.531722\n","  Val Acc: 0.683721   Val Loss: 0.884015\n","==================== Epoch 98 ====================\n","Train Acc: 0.759073 Train Loss: 0.576045\n","  Val Acc: 0.700000   Val Loss: 0.856593\n","==================== Epoch 100 ====================\n","Train Acc: 0.783012 Train Loss: 0.535241\n","  Val Acc: 0.686047   Val Loss: 0.860346\n","==================== Epoch 102 ====================\n","Train Acc: 0.790734 Train Loss: 0.518233\n","  Val Acc: 0.706977   Val Loss: 0.849042\n","==================== Epoch 104 ====================\n","Train Acc: 0.800000 Train Loss: 0.516018\n","  Val Acc: 0.706977   Val Loss: 0.848577\n","==================== Epoch 106 ====================\n","Train Acc: 0.794595 Train Loss: 0.483744\n","  Val Acc: 0.693023   Val Loss: 1.003969\n","==================== Epoch 108 ====================\n","Train Acc: 0.789189 Train Loss: 0.503979\n","  Val Acc: 0.723256   Val Loss: 0.826219\n","==================== Epoch 110 ====================\n","Train Acc: 0.797683 Train Loss: 0.520542\n","  Val Acc: 0.693023   Val Loss: 0.857486\n","==================== Epoch 112 ====================\n","Train Acc: 0.758301 Train Loss: 0.616013\n","  Val Acc: 0.704651   Val Loss: 0.874563\n","==================== Epoch 114 ====================\n","Train Acc: 0.783784 Train Loss: 0.530078\n","  Val Acc: 0.720930   Val Loss: 0.860309\n"]}],"source":["# hyper parameters\n","max_epochs = 200\n","log_interval = 2 \n","\n","\n","train_acc_list = []\n","train_loss_list = []\n","val_acc_list = []\n","val_loss_list = []      \n","      \n","for epoch in range(1, max_epochs + 1):\n","    train_acc, train_loss = train(train_loader, model, criterion, optimizer)\n","    val_acc, val_loss = val(val_loader, model, criterion)\n","    train_acc_list.append(train_acc)\n","    train_loss_list.append(train_loss)\n","    val_acc_list.append(val_acc)   \n","    val_loss_list.append(val_loss)\n","\n","    # print acc and loss in per log_interval time          \n","    if epoch % log_interval == 0:    \n","        print('=' * 20, 'Epoch', epoch, '=' * 20)      \n","        print('Train Acc: {:.6f} Train Loss: {:.6f}'.format(train_acc, train_loss)) \n","        print('  Val Acc: {:.6f}   Val Loss: {:.6f}'.format(val_acc, val_loss)) \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sdBlib2wzYU7"},"outputs":[],"source":["# save well-trained state_dict of model          \n","torch.save(model.state_dict(), 'NAME_OF_THIS_EXPERIMENT.pt') "]},{"cell_type":"markdown","metadata":{"id":"W5pW9zcKAN-2"},"source":["#### Visualize accuracy and loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4ifzgfp7iq2m"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","plt.figure(figsize=(12, 4))\n","plt.plot(range(len(train_loss_list)), train_loss_list)\n","plt.plot(range(len(val_loss_list)), val_loss_list, c='r')\n","plt.legend(['train', 'val'])\n","plt.title('Loss')\n","plt.show()\n","plt.figure(figsize=(12, 4))\n","plt.plot(range(len(train_acc_list)), train_acc_list)\n","plt.plot(range(len(val_acc_list)), val_acc_list, c='r')\n","plt.legend(['train', 'val'])\n","plt.title('Acc')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"zMYSgO5viYOk"},"source":["### Prediction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mHc4hsPMzYU8"},"outputs":[],"source":["## load previous best model\n","# ckpt = torch.load('NAME_OF_THIS_EXPERIMENT.pt')\n","# model.load_state_dict(ckpt) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SiyK25P6KXrn"},"outputs":[],"source":["def predict(input_data, model):\n","    model.eval()\n","    output_list = []\n","    with torch.no_grad():\n","        for images in input_data:\n","            images = images.to(device)\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            output_list.extend(predicted.to('cpu').numpy().tolist())\n","    return output_list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0I0LN7HwpnsX"},"outputs":[],"source":["idx = 0\n","output_csv = predict(test_loader, model)\n","with open('result.csv', 'w', newline='') as csvFile:\n","    writer = csv.DictWriter(csvFile, fieldnames=['file_path', 'label'])\n","    writer.writeheader()\n","    for result in output_csv:\n","        file_path = dataset_test.data_list[idx].replace(data_folder + '/', '')\n","        writer.writerow({'file_path':file_path, 'label':result})\n","        idx += 1"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Assign_3_flower_classification.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
